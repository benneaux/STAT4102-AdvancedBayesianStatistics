---
title: "Assignment 2"
date: "`r Sys.Date()`"
output:  pdf_document
---

```{r DocSetup, include = FALSE}
require(knitr)
opts_chunk$set(eval = FALSE, echo = FALSE, warning = FALSE)
opts_chunk$set(fig.width = 8, fig.height = 6, fig.align = "center")
```



```{r Q1Setup, include = FALSE}
source("Question1/1setup.R")
```


## Question 1: Inference for the Poisson parameter $\lambda$:
(a) Given a general Gamma(a,b) prior, derive the posterior distribution of $\lambda$, given data $(x_1,\dots,x_n)$, followed by the posterior predictive distribution of $(z_1,\dots,z_m)$.  

The Likelihood is given by:
$$
L(\lambda|x) = \prod\limits_{i=1}^{n}\frac{e^{-\lambda}\lambda^{x_i}}{x_i!}=\frac{e^{-n\lambda}\lambda^{\sum x_i}}{\prod_{i=1}^{n}(x_i!)}
$$

The Prior is a $Gamma(a,b)$:
$$
p(\lambda) = \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta \lambda}, \lambda > 0
$$
So the posterior is given by:
$$
p(\lambda|x)\propto\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\sum x_i+\alpha-1}e^{-(n+\beta)\lambda}, \lambda > 0,
$$
which is equivalent to a $Gamma(\sum x_i + \alpha, n + \beta)$, or $Gamma(n\bar{x} + \alpha, n + \beta)$.

The \emph{Posterior Predictive Distribution} for a Poisson parameter $\lambda$ with data $(z_1,\dots,z_m)$ is given by:


$$\begin{aligned}
p(z|X) &= \int\limits_0^\infty p(z|\lambda)p(\lambda | X)d\lambda \\
&=\int\limits_0^\infty Poisson(z|\lambda) \cdot Gamma(\sum x_i + \alpha, n + \beta) d\lambda \\
&=\int\limits_0^\infty \frac{e^{-\lambda}\lambda^{z}}{z!} \cdot \frac{(n+\beta)^{\sum x_i +\alpha}}{\Gamma(\sum x_i + \alpha)}\lambda^{\sum x_i + \alpha-1}e^{-(n + \beta)\lambda} d\lambda \\
&= \frac{(n+\beta)^{\sum x_i +\alpha}}{\Gamma(\sum x_i + \alpha)\Gamma(z + 1)}\int\limits_0^\infty \lambda^{z + \sum x_i + \alpha-1}e^{-(n + \beta + 1)\lambda} d\lambda \\
&= \frac{(n+\beta)^{\sum x_i +\alpha}}{\Gamma(\sum x_i + \alpha)\Gamma(z + 1)} \cdot \frac{\Gamma(z + \sum x_i + \alpha)}{(n+\beta + 1)^{z \sum x_i +\alpha}} \\
&= \frac{\Gamma(z + \sum x_i + \alpha)}{\Gamma(\sum x_i + \alpha)\Gamma(z + 1)} \left( \frac{n + \beta}{n + \beta + 1} \right)^{\sum x_i + \alpha} \left(\frac{1}{n + \beta + 1} \right)^{z}
\end{aligned}$$

\
  (b) Assuming Gamma(a,b) priors for two Poisson parameters $\lambda_1$ and $\lambda_2$, derive the posterior for $\phi = \lambda_1 / \lambda_2$. (Hint: use nuisance parameter $\mu = \lambda_2$).
  
  (b) Assuming Gamma(a,b) priors for two Poisson parameters $\lambda_1$ and $\lambda_2$, derive the posterior for $\phi = \lambda_1 / \lambda_2$. (Hint: use nuisance parameter $\mu = \lambda_2$).
  
Firstly, the likelihood function for two Poissons is: 
$$
p(x_1, x_2 | \lambda_1, \lambda_2) = p(x_1|\lambda_1)p(x_2|\lambda_2) = \frac{e^{-\lambda_1}\lambda_1^{x_1}}{x_1!} \cdot \frac{e^{-\lambda_2}\lambda_2^{x_2}}{x_2!}.
$$
Reparameterising by $\phi = \frac{\lambda_1}{\lambda_2}$, we get:  
$$
p(x_1, x_2 | \phi, \lambda_2) = \frac{e^{-\phi \lambda_2}(\phi \lambda_2)^{x_1}}{x_1!} \cdot \frac{e^{-\lambda_2}\lambda_2^{x_2}}{x_2!}.
$$
We then compute the Fisher Information Matrix:

$$
\begin{aligned}
I(\theta)_{ij} &= E\left(-\frac{\partial^2l}{\partial \theta_i \partial \theta_j}\right) \\
\implies F(\phi, \lambda_2) &= 
\begin{bmatrix}
\frac{\lambda_2}{\phi} & 1 \\
1 & \frac{1 + \phi}{\lambda_2}
\end{bmatrix} \\
\implies S(\phi, \lambda_2) = F^{-1}(\phi, \lambda_2) &= \begin{bmatrix}
\frac{\phi(1 + \phi)}{\lambda_2} & -\phi \\
-\phi & \lambda_2
\end{bmatrix}
\end{aligned}
$$

Following the algorithm laid out by Barnardo we can define the marginal and conditional asympototic posteriors for $\phi$.

$$
\begin{aligned}
d_0(\phi, \lambda_2) &= \left[\frac{\phi(1 + \phi)}{\lambda_2}\right]^{1/2} \\
d_1(\phi, \lambda_2) &= \left(\frac{\lambda_2}{1 + \phi}\right)^{1/2}
\end{aligned}
$$

According to Corollary 1 of Proposition 2 in Barnardo's paper, because the nuisance parameter space $\Lambda(\phi) =\Lambda$ is independent of $\phi$, we can factorise the above equations as
$$
\begin{aligned}
d_0^{-1}(\phi, \lambda_2) &= \frac{1}{\sqrt{\phi(1 + \phi)}}\cdot \sqrt{\lambda_2} = a_0(\phi)b_0(\lambda_2) \\
d_1^{-1}(\phi, \lambda_2) &= \sqrt{\phi(1 + \phi)}\cdot \frac{1}{\sqrt{\lambda_2}} = a_1(\phi)b_1(\lambda_2)
\end{aligned}
$$
which implies that the marginal and conditional reference priors are

$$
\begin{aligned}
\pi(\phi) \propto a_0(\phi) &= \frac{1}{\sqrt{\phi(1 + \phi)}} \\
\pi(\lambda|\phi) \propto b_1(\lambda_2) &= \frac{1}{\sqrt{\lambda_2}}
\end{aligned}
$$

The joint posterior can be derived with the likelihood and joint prior $(d_1^{-1}(\phi, \lambda_2))$ previously derived.

$$
\begin{aligned}
\pi(\phi,\lambda_2|x_1, x_2) &\propto \pi(x_1,x_2|\phi, \lambda_2)\cdot\pi(\phi,\lambda_2) \\
&\propto e^{-(\phi + 1)\lambda_2}\cdot \phi^{x_1-1/2}\left( 1 + \phi \right)^{1/2} \cdot \lambda_2^{x_1 + x_2 - 1/2}
\end{aligned}
$$

which, I'm pretty sure, can be factored as 

$$
\pi(\phi,\lambda_2|x_1, x_2) \propto Gamma(\lambda_2|x_1 + 1/2,1)\cdot Gamma\left(\phi|x_1, \frac{1}{\lambda_2}\right)\cdot Beta\left(\frac{\phi}{1 + \phi}|3/2,1 \right).
$$

(c) Derive the Jeffreys prior for $(\phi, \mu)$, and the corresponding marginal posterior for $\phi$.

The Jeffreys principle states that the Jeffreys prior is 

$$
\pi_\phi(\phi) \propto det(I(\phi))^{1/2} = \left(\frac{\lambda_2(1+\phi)}{\phi} - 1\right)^{-1/2} = \phi^{-1/2}
$$

```{r Question1cData, echo = TRUE, cache = TRUE}
```

(d) Derive the reference prior for $(\phi, \mu)$, and the corresponding marginal posterior for $\phi$. 

```{r Question1d}
```

(e) Based on (c), consider the posterior for $\phi$ based on uniform priors for $\lambda_1$ and $\lambda_2$, and comment on when inference based on this posterior could be quite different from that based on the reference posterior from (e). Give a data example, in terms of resulting intervals. (Hint: the above posteriors are related to known pdfs, and by transformation may be simplified even further.).


```{r Question1e, cache = TRUE, echo = FALSE}
```

```{r Question1e2, cache = FALSE, fig.height = 10, fig.width  = 8, dependson = "Question1e"}
```

\newpage

```{r Q2Setup, include = FALSE}
rm(list = ls())
source("Question2/2setup.R")
opts_chunk$set(warning = FALSE, echo = TRUE, cache = TRUE)
opts_chunk$set(fig.width = 8, fig.height = 6, fig.align = "center")
```


## Question 2: Inference for variance components: 

(a) Use e.g. SAS (PROC VARCOMP) to perform a classical analysis of the data in Table 5.1.4 of Box & Tiao (1973), based on finding point estimates only.

```{r Question2a, eval = FALSE}
source("Question2/2setup.R")
library(lme4)
data("Dyestuff2")
fit <- aov(Yield ~ Batch, data= Dyestuff2) 
summary(fit)
coefficients(fit)
proj(fit, onedf = TRUE)
library(varComp)
library(nlme)
lmef <- lme(Yield ~ 1, Dyestuff2, ~1|Batch)
summary(lmef)
vcf <- varComp(Yield ~ Batch, Dyestuff2, )
summary(vcf)
VarCorr(lmef)
coef(vcf, 'varComp')
logLik(vcf)
vcf0 = varComp(Yield~Batch, Dyestuff2)
vcf1 = varComp(Yield~1, Dyestuff2)
coef(vcf0,"varComp")
coef(vcf1,"varComp")
logLik(vcf0)

```
(b)  Use WinBUGS for a Bayesian analysis of (a), and find reasonable point and interval estimates for $\sigma_1^2$ and $\sigma_2^2$. Include graphs, including one of the joint posterior. [6 marks] 

```{r Question2b, eval=FALSE}
```
```{r Q3BUGS, cache=FALSE}
#define the model
library(R2OpenBUGS)
dyesmodel <- function(){
  for(i in 1 : batches) {
    m[i] ~ dnorm(theta,  tau.btw)
    for(j in 1 : samples) {
     y[j , i] ~ dnorm(m[i], tau.with)
    }
  }
  sigma2.with <- 1/tau.with
  tau.with ~ dgamma(0.001, 0.001)
  
  ICC ~ dunif(0,1)
  sigma2.btw <- sigma2.with*(ICC/(1-ICC))
  tau.btw <- 1/sigma2.btw
  theta ~ dflat()
  }
# write the model code out to a file
write.model(dyesmodel, "dyesmodel.txt")
model.file1 = paste(getwd(),"dyesmodel.txt", sep="/")
library(lme4)
data("Dyestuff2")
#prepare the data for input into OpenBUGS
batches <- 6
samples <- 5
y <- tbl_df(Dyestuff2) %>%
  mutate(samp = rep(seq(1,5,1),6)) %>%
  spread(Batch, Yield) %>%
  dplyr::select(-1)
y <- as.matrix(y)
  
data <- list ("batches","samples","y")

#initialization of variables
inits <- function(){
  list(theta=5, tau.with=1, ICC=0.5)
}

WINE="/opt/local/bin/wine"
WINEPATH="/opt/local/bin/winepath"
OpenBUGS.pgm=paste0("/Users/benjamin/Applications/wine/",
                    "drive_c/ProgramFiles/OpenBUGS/OpenBUGS323/OpenBUGS.exe")

#these are the parameters to save
parameters = c("sigma2.with","theta","sigma2.btw")

#run the model
dyes.sim <- bugs(data, 
                 inits, 
                 model.file = model.file1,
                 parameters=parameters,
                 n.burnin = 2000,
                 n.chains = 5, 
                 n.iter = 30000, 
                 OpenBUGS.pgm=OpenBUGS.pgm, 
                 WINE=WINE, 
                 WINEPATH=WINEPATH,
                 useWINE=T,
                 codaPkg = T,
                 debug = T)
# print(dyes.sim)
# dyes.sim$summary

out.coda <- read.bugs(dyes.sim)
library(coda)
library(lattice)
plot(out.coda)
HPDinterval(out.coda)
batchSE(out.coda)
densityplot(out.coda)
acfplot(out.coda)
gelman.diag(out.coda)
gelman.plot(out.coda)
out.summary <- summary(out.coda, q=c(0.025, 0.975))
out.summary
out.summary$stat["sigma2.with",]
# out.summary$q["sigma2.with", ]
out.summary$stat["sigma.btw",]
# out.summary$q["sigma2.btw", ]
out.summary$stat["theta",]
# out.summary$q["theta", ]
```


(c) Box & Tiao also studied a 3-component model (Table 5.3.1). 
    i. Derive central credible intervals, for the 3 individual components, based on Table 5.3.3.
    ii. Use WinBUGS to do the same, and include graphs.
    iii. While not going as far as Box & Tiaoâ€™s Figure 5.3.2, produce a graph of the joint posterior of $\sigma_2^2$ and $\sigma_3^2$, and one of $\sigma_2^2 / \sigma_3^2$

```{r Question2c}
dfI <-  rep(seq(1,10),4)
dfJ <- c(rep(1,20),rep(2,20))
dfK <- c(rep(1,10),rep(2,10),rep(1,10),rep(2,10))
df <- cbind(dfI,dfJ,dfK)

dfJ1K1 <- c(2.004,4.342,0.869,3.531,2.579,-1.404,-1.676,1.670,2.141,-1.229)
dfJ1K2 <- c(2.713,4.229,-2.621,4.185,4.271,-1.003,-0.208,2.426,3.527,-0.596)
dfJ2K1 <- c(0.602,3.344,-3.896,1.722,-2.101,-0.755,-9.139,1.834,0.462,4.471)
dfJ2K2 <- c(0.252,3.057,-3.696,0.380,0.651,-2.202,-8.653,1.200,0.665,1.606)
df2 <- c(dfJ1K1,dfJ1K2,dfJ2K1,dfJ2K2)

df3 <- cbind(df,df2)
colnames(df3) <- c("I","J","K","Sample")
df3 <- as.data.frame(df3) %>%
  mutate(I = as.factor(I)) %>%
  mutate(J = as.factor(J)) %>%
  mutate(K = as.factor(K))


#define the model
library(R2OpenBUGS)
compmodel <- function(){
  for(i in 1 : I) {
    m[i] ~ dnorm(theta, tau.a)
    for(j in 1 : J) {
      y[i , j] ~ dnorm(m[i], tau.b)
      for(k in 1 : K) {
      y[i,j,k] ~ dnorm(y[i,j], tau.c)
      }
      }
    
  }
  sigma2.a <- 1 / tau.b
  sigma2.b <- 1 / tau.b
  sigma2.c <- 1 / tau.c
  tau.a ~ dgamma(0.001, 0.001)
  tau.b ~ dgamma(0.001, 0.001)
  tau.c ~ dgamma(0.001, 0.001)
  theta ~ dflat()
}
# write the model code out to a file
write.model(compmodel, "comp.txt")
model.file1 = paste(getwd(),"comp.txt", sep="/")

y <- tbl_df(df3) %>%
   spread("J") %>%
   dplyr::select(-1)
y <- as.matrix(y)  
data <- list("I","J","K","Sample")

#initialization of variables
inits <- function(){
  list(theta=1, tau.a=1, tau.b=1,tau.c=1)
}

WINE="/opt/local/bin/wine"
WINEPATH="/opt/local/bin/winepath"
OpenBUGS.pgm=paste0("/Users/benjamin/Applications/wine/",
                    "drive_c/ProgramFiles/OpenBUGS/OpenBUGS323/OpenBUGS.exe")

#these are the parameters to save
parameters = c("sigma2.a","sigma2.b","sigma2.c","theta")

#run the model
comp.sim <- bugs(data, 
                 inits, 
                 model.file = model.file1,
                 parameters=parameters,
                 n.burnin = 2000,
                 n.chains = 5, 
                 n.iter = 15000, 
                 OpenBUGS.pgm=OpenBUGS.pgm, 
                 WINE=WINE, 
                 WINEPATH=WINEPATH,
                 useWINE=T,
                 codaPkg = T,
                 debug = F)
# print(dyes.sim)
# dyes.sim$summary

out.coda <- read.bugs(comp.sim)
library(coda)
library(lattice)
plot(out.coda)
HPDinterval(out.coda)
batchSE(out.coda)
densityplot(out.coda)
acfplot(out.coda)
gelman.diag(out.coda)
gelman.plot(out.coda)
out.summary <- summary(out.coda, q=c(0.025, 0.975))
out.summary
out.summary$stat["sigma2.a",]
# out.summary$q["sigma2.with", ]
out.summary$stat["sigma2.b",]
out.summary$stat["sigma2.c",]
# out.summary$q["sigma2.btw", ]
out.summary$stat["theta",]
# out.summary$q["theta", ]



# 
# 
# library(lme4)
# library(varComp)
# library(nlme)
# lme(distance ~ age, data = Orthodont)
# lmef <- lme(Sample ~ 1, data = df3, ~1|I/J/K)
# summary(lmef)
# vcf <- varComp(Sample~K, df3,~I/J)
# summary(vcf)
# coef(vcf, 'varComp')
# k0=tcrossprod(model.matrix(~0+I,df3))
# k1=tcrossprod(df3$J==1)*k0
# k2=tcrossprod(df3$J==2)*k0
# k3=tcrossprod(model.matrix(~0+I:K, df3))
# ## unequal variance across Source for Lot effects, in a preferred parameterization:
# (vcf1 = varComp(Sample~1, df3, varcov=list(S1Lot=k1, S2Lot=k2, S3Lot=k3))) 
# ## unequal variance across Source for Lot effects, in a different parameterization:
# (vcf2 = varComp(Sample~J, df3, varcov=list(Lot=k0, S2Lot=k2, `Lot:K`=k3)))
# ## unequal variance across Source for Lot effects, but in a poor parameterization that 
# ##   turns out to be the same as vcf after fitting.
# (vcf3 = varComp(Sample~J, df3, varcov=list(Lot=k0, S1Lot=k1, `Lot:K`=k3)))  
# logLik(vcf)
# logLik(vcf1)
# logLik(vcf2)  ## the same as vcf1
# logLik(vcf3)  ## the same as vcf
# 
# 
# vcf0 = varComp(Sample~I, df3)
# vcf1 = varComp(Yield~Batch, Dyestuff2, ~Yield/Batch)
# summary(vcf0)
# vcf1
# logLik(vcf0)
# library(hglm)
# m11 <- hglm(fixed = Sample ~ 1 +J + K,
#             random = ~ 1|I,
#             rand.family = gaussian(link = "identity"),
#             disp = ~ J + K,
#             data = df3)
# summary(m11)
# print(summary(m11), print.ranef = TRUE)
```


(d) Box, Hunter & Hunter (1976, Chapter 17.3) studied a pigment paste example with three components, focusing on point estimates only. Use WinBUGS again to perform a Bayesian analysis. Include graphs.

```{r Question2d}
### Oxide/Semiconductor data example
library(nlme)
data(Oxide)
lmef = lme(Thickness~Source, Oxide, ~1|Lot/Wafer)
vcf = varComp(Thickness~Source, Oxide, ~Lot/Wafer)
VarCorr(lmef)
coef(vcf, 'varComp') ## same values as above
k0=tcrossprod(model.matrix(~0+Lot,Oxide))
k1=tcrossprod(Oxide$Source==1)*k0
k2=tcrossprod(Oxide$Source==2)*k0
k3=tcrossprod(model.matrix(~0+Lot:Wafer, Oxide))
## unequal variance across Source for Lot effects, in a preferred parameterization:
(vcf1 = varComp(Thickness~Source, Oxide, varcov=list(S1Lot=k1, S2Lot=k2, `Lot:Wafer`=k3))) 
## unequal variance across Source for Lot effects, in a different parameterization:
(vcf2 = varComp(Thickness~Source, Oxide, varcov=list(Lot=k0, S2Lot=k2, `Lot:Wafer`=k3)))
## unequal variance across Source for Lot effects, but in a poor parameterization that 
##   turns out to be the same as vcf after fitting.
(vcf3 = varComp(Thickness~Source, Oxide, varcov=list(Lot=k0, S1Lot=k1, `Lot:Wafer`=k3)))  
logLik(vcf)
logLik(vcf1)
logLik(vcf2)  ## the same as vcf1
logLik(vcf3)  ## the same as vcf
## fixef-effect only
vcf0 = varComp(Thickness~Source, Oxide)
summary(vcf0)
summary(lmf<-lm(Thickness~Source, Oxide))
vcf00 = varComp(Thickness~0, Oxide)
summary(vcf00)
summary(lmf0<-lm(Thickness~0, Oxide))


```


