---
title: "Assignment 2"
date: "`r Sys.Date()`"
output:  
  pdf_document:
    dev: png
geometry: margin=1cm

---

```{r DocSetup, include = FALSE}
require(knitr)
opts_chunk$set(warning = FALSE, cache = TRUE)
opts_chunk$set(fig.height = 3, fig.width = 7,fig.align = "center")
```



```{r Q1Setup, include = FALSE}
source("Question1/1setup.R")
```


## Question 1: Inference for the Poisson parameter $\lambda$:
(a) Given a general Gamma(a,b) prior, derive the posterior distribution of $\lambda$, given data $(x_1,\dots,x_n)$, followed by the posterior predictive distribution of $(z_1,\dots,z_m)$.  

The Likelihood is given by:
$$
L(\lambda|x) = \prod\limits_{i=1}^{n}\frac{e^{-\lambda}\lambda^{x_i}}{x_i!}=\frac{e^{-n\lambda}\lambda^{\sum x_i}}{\prod_{i=1}^{n}(x_i!)}
$$

The Prior is a $Gamma(a,b)$:
$$
p(\lambda) = \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta \lambda}, \lambda > 0
$$
So the posterior is given by:
$$
p(\lambda|x)\propto\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\sum x_i+\alpha-1}e^{-(n+\beta)\lambda}, \lambda > 0,
$$
which is equivalent to a $Gamma(\sum x_i + \alpha, n + \beta)$, or $Gamma(n\bar{x} + \alpha, n + \beta)$.

The \emph{Posterior Predictive Distribution} for a Poisson parameter $\lambda$ with data $(z_1,\dots,z_m)$ is given by:


$$\begin{aligned}
p(z|X) &= \int\limits_0^\infty p(z|\lambda)p(\lambda | X)d\lambda \\
&=\int\limits_0^\infty Poisson(z|\lambda) \cdot Gamma(\sum x_i + \alpha, n + \beta) d\lambda \\
&=\int\limits_0^\infty \frac{e^{-\lambda}\lambda^{z}}{z!} \cdot \frac{(n+\beta)^{\sum x_i +\alpha}}{\Gamma(\sum x_i + \alpha)}\lambda^{\sum x_i + \alpha-1}e^{-(n + \beta)\lambda} d\lambda \\
&= \frac{(n+\beta)^{\sum x_i +\alpha}}{\Gamma(\sum x_i + \alpha)\Gamma(z + 1)}\int\limits_0^\infty \lambda^{z + \sum x_i + \alpha-1}e^{-(n + \beta + 1)\lambda} d\lambda \\
&= \frac{(n+\beta)^{\sum x_i +\alpha}}{\Gamma(\sum x_i + \alpha)\Gamma(z + 1)} \cdot \frac{\Gamma(z + \sum x_i + \alpha)}{(n+\beta + 1)^{z \sum x_i +\alpha}} \\
&= \frac{\Gamma(z + \sum x_i + \alpha)}{\Gamma(\sum x_i + \alpha)\Gamma(z + 1)} \left( \frac{n + \beta}{n + \beta + 1} \right)^{\sum x_i + \alpha} \left(\frac{1}{n + \beta + 1} \right)^{z}
\end{aligned}$$

\
  (b) Assuming Gamma(a,b) priors for two Poisson parameters $\lambda_1$ and $\lambda_2$, derive the posterior for $\phi = \lambda_1 / \lambda_2$. (Hint: use nuisance parameter $\mu = \lambda_2$).
  
  
Firstly, the likelihood function for two Poissons is: 
$$
p(x_1, x_2 | \lambda_1, \lambda_2) = p(x_1|\lambda_1)p(x_2|\lambda_2) = \frac{e^{-\lambda_1}\lambda_1^{x_1}}{x_1!} \cdot \frac{e^{-\lambda_2}\lambda_2^{x_2}}{x_2!}.
$$
Reparameterising by $\phi = \frac{\lambda_1}{\lambda_2}$, we get:  
$$
p(x_1, x_2 | \phi, \lambda_2) = \frac{e^{-\phi \lambda_2}(\phi \lambda_2)^{x_1}}{x_1!} \cdot \frac{e^{-\lambda_2}\lambda_2^{x_2}}{x_2!}.
$$
We then compute the Fisher Information Matrix:

$$
\begin{aligned}
I(\theta)_{ij} &= E\left(-\frac{\partial^2l}{\partial \theta_i \partial \theta_j}\right) \\
\implies F(\phi, \lambda_2) &= 
\begin{bmatrix}
\frac{\lambda_2}{\phi} & 1 \\
1 & \frac{1 + \phi}{\lambda_2}
\end{bmatrix} \\
\implies S(\phi, \lambda_2) = F^{-1}(\phi, \lambda_2) &= \begin{bmatrix}
\frac{\phi(1 + \phi)}{\lambda_2} & -\phi \\
-\phi & \lambda_2
\end{bmatrix}
\end{aligned}
$$

Following the algorithm laid out by Bernardo we can define the marginal and conditional asympototic posteriors for $\phi$.

$$
\begin{aligned}
d_0(\phi, \lambda_2) &= \left[\frac{\phi(1 + \phi)}{\lambda_2}\right]^{1/2} \\
d_1(\phi, \lambda_2) &= \left(\frac{\lambda_2}{1 + \phi}\right)^{1/2}
\end{aligned}
$$

According to Corollary 1 of Proposition 2 in Barnardo's paper, because the nuisance parameter space $\Lambda(\phi) =\Lambda$ is independent of $\phi$, we can factorise the above equations as
$$
\begin{aligned}
d_0^{-1}(\phi, \lambda_2) &= \frac{1}{\sqrt{\phi(1 + \phi)}}\cdot \sqrt{\lambda_2} = a_0(\phi)b_0(\lambda_2) \\
d_1^{-1}(\phi, \lambda_2) &= \sqrt{\phi(1 + \phi)}\cdot \frac{1}{\sqrt{\lambda_2}} = a_1(\phi)b_1(\lambda_2)
\end{aligned}
$$
which implies that the marginal and conditional reference priors are

$$
\begin{aligned}
\pi(\phi) \propto a_0(\phi) &= \frac{1}{\sqrt{\phi(1 + \phi)}} \\
\pi(\lambda|\phi) \propto b_1(\lambda_2) &= \frac{1}{\sqrt{\lambda_2}}
\end{aligned}
$$

The joint posterior can be derived with the likelihood and joint prior $(d_1^{-1}(\phi, \lambda_2))$ previously derived.

$$
\begin{aligned}
\pi(\phi,\lambda_2|x_1, x_2) &\propto \pi(x_1,x_2|\phi, \lambda_2)\cdot\pi(\phi,\lambda_2) \\
&\propto e^{-(\phi + 1)\lambda_2}\cdot \phi^{x_1-1/2}\left( 1 + \phi \right)^{1/2} \cdot \lambda_2^{x_1 + x_2 - 1/2}
\end{aligned}
$$

which, I'm pretty sure, can be factored as 

$$
\pi(\phi,\lambda_2|x_1, x_2) \propto Gamma(\lambda_2|x_1 + 1/2,1)\cdot Gamma\left(\phi|x_1, \frac{1}{\lambda_2}\right)\cdot Beta\left(\frac{\phi}{1 + \phi}|3/2,1 \right).
$$

(c) Derive the Jeffreys prior for $(\phi, \mu)$, and the corresponding marginal posterior for $\phi$.

The Jeffreys principle states that the Jeffreys prior is 

$$
\pi_\phi(\phi) \propto det(I(\phi))^{1/2} = \left(\frac{\lambda_2(1+\phi)}{\lambda_2\phi} - 1\right)^{1/2} = \phi^{-1/2}
$$

The marginal posterior can be derived by multiplyin the likelihood by the Jeffrey's prior and then integrating out the nuissance parameter:

$$
\begin{aligned}
\pi(\phi|x_1,x_2) &\propto  \int_{0}^\infty \frac{e^{-\phi \lambda_2}(\phi \lambda_2)^{x_1}}{x_1!} \cdot \frac{e^{-\lambda_2}\lambda_2^{x_2}}{x_2!}\cdot\frac{1}{\sqrt{\phi}} \quad d\lambda_2 \\
&\propto \phi^{x_1 -1/2} \int_{0}^\infty e^{-(\phi+1)\lambda_2} \cdot \lambda_2^{x_1 + x_2}\quad  d\lambda_2
\end{aligned}
$$
Here we solve the integral:

$$
\int_{0}^\infty e^{-(\phi+1)\lambda_2} \cdot \lambda_2^{x_1 + x_2}\quad  d\lambda_2 = (\phi + 1)^{-(x_1 + x_2 + 1)}\cdot \Gamma(x_1 + x_2 + 1)
$$
which leads us to

$$
\begin{aligned}
\pi(\phi|x_1,x_2) &\propto \phi^{x_1 -1/2} (\phi + 1)^{-(x_1 + x_2 + 1)}\cdot \Gamma(x_1 + x_2 + 1) \\
&\propto \frac{\phi^{x_1 -1/2}}{(\phi + 1)^{-(x_1 + x_2 + 1)}}
\end{aligned}
$$

(d) Derive the reference prior for $(\phi, \mu)$, and the corresponding marginal posterior for $\phi$. 

We can combine the marginal and conditional refererence priors derived above to determine the joint reference prior.

$$
\pi(\phi, \lambda_2) = \pi(\phi)\pi(\lambda_2|\phi) = \frac{1}{\sqrt{\phi(1 + \phi)}}\cdot\frac{1}{\sqrt{\lambda_2}}
$$
Now that we have the marginal reference prior for $\phi$, Bernardo states that we can calculate the marginal posterior of $\phi$ by integrating out the nuisance parameter $\lambda_2$.
$$
\begin{aligned}
\pi(\phi|x_1,x_2) &= \pi(\phi) \int_{\Lambda(\phi)}p(x_1,x_2|\phi, \lambda_2)\pi(\lambda_2|\phi)d\lambda_2 \\
&= \pi(\phi) \int_{\Lambda} \frac{e^{-\phi \lambda_2}(\phi \lambda_2)^{x_1}}{x_1!} \cdot \frac{e^{-\lambda_2}\lambda_2^{x_2}}{x_2!}\cdot\frac{1}{\sqrt{\lambda_2}} d\lambda_2 \\
&\propto \frac{1}{\sqrt{\phi(1 + \phi)}} \int_{\Lambda} e^{-(\phi+1)\lambda_2}\phi^{x_1} \cdot \frac{\lambda_2^{x_1 + x_2}}{\sqrt{\lambda_2}} d\lambda_2 \\
&\propto \frac{\phi^{x_1}}{\sqrt{\phi(1 + \phi)}} \int_{\Lambda} e^{-(\phi+1)\lambda_2} \cdot \frac{\lambda_2^{x_1 + x_2}}{\sqrt{\lambda_2}} d\lambda_2
\end{aligned}
$$

Here we solve the integral

$$
\int_{\Lambda} e^{-(\phi+1)\lambda_2} \cdot \frac{\lambda_2^{x_1 + x_2}}{\sqrt{\lambda_2}} d\lambda_2 = \frac{\Gamma(x_1 + x_2 - 1/2)}{(1+\phi)^{(x_1 + x_2 + 1/2)}}
$$

which leaves
$$
\pi(\phi|x_1,x_2) \propto \frac{\phi^{x_1 - 1/2}}{(1 + \phi)^{(x_1 + x_2 +1)}} 
$$
which is identical to the marginal posterior derived by using the Jeffrey's prior.

(e) Based on (c), consider the posterior for $\phi$ based on uniform priors for $\lambda_1$ and $\lambda_2$, and comment on when inference based on this posterior could be quite different from that based on the reference posterior from (d). Give a data example, in terms of resulting intervals. (Hint: the above posteriors are related to known pdfs, and by transformation may be simplified even further.).

Before I begin, I should note that you deal with a lot of these issues yourself Frank, in \emph{Consensus priors for multinomial and binomial ratios}.

Firstly, the parameter $\phi$ can be regarded as a multinomial ratio, a link established by both yourself -- in \emph{Section 2.3: Straitforward alternative derivations} -- and Bernardo, who uses the transformation $\omega = \frac{phi}{1+phi}$ to arrive at an alternative expression for the marginal posterior density function given above:

$$
\\pi(\phi|x_1,x_2)| x_1, x_2)\left|\frac{d\phi}{d\omega}\right| = \pi(\omega|x_1,x_2) \propto \omega^{x_1 - 1/2}(1-\omega)^{x_2-1/2} =  Beta(\omega|x_1 + 1/2, x_2 + 1/2)
$$

Given this is a multinomial case where the Jeffrey's/Reference posterior is given above, you give the Bayes posterior as 

$$
\pi_B(\phi|x_1,x_2) \propto \frac{\phi^{x_1}}{(1 + \phi)^{(x_1 + x_2 + 2)}}
$$ 
which we can derive a similar alternative expression for: 

$$
\\pi(\phi|x_1,x_2)| x_1, x_2)\left|\frac{d\phi}{d\omega}\right| = \pi(\omega|x_1,x_2) \propto \omega^{x_1}(1-\omega)^{x_2} =  Beta(\omega|x_1 + 1, x_2 + 1)
$$

As usual, the conditions where the difference between the Bayes and Jeffrey's/Reference marginal posteriors becomes apparent is when $x_1 = 0, x_2 \to n$. The Jeffrey's/Reference posterior will have more weight closer to zero than the BL posterior, which we studied more closely in Assignment 1. Also, we know that in terms of mean coverage, BL $\to 1-\alpha, n\to \infty$, which is not case for J/R. 

In order to represent the performance of each posterior in the case of $x_n = 0$, I've reused some code from my previous Assignmet.

```{r, echo = FALSE}
blcover0 <- function(p,x = 0,...) {
  dens = dbinom(x,n,p)
  hig = solve.HPD.beta(shape1 = 1, shape2 = n + 1,...)
  intvals = as.numeric(p <= hig)
  sum(intvals * dens)
}

jeffreyscover0 <- function(p,x = 0,...) {
  dens = dbinom(x,n,p)
  hig = solve.HPD.beta(shape1 = 0.5, shape2 = n + 0.5,...)
  intvals = as.numeric(p <= hig)
  sum(intvals * dens)
}


solve.HPD.beta = function(shape1, shape2, credint = 0.95, one.sided = FALSE,...){
  if(shape1 <= 1| one.sided == TRUE){
    lt = 0
    ut = qbeta(credint, shape1, shape2)
    coverage = credint
    results = data_frame("Lower"    = lt,
                         "Upper"    = ut,
                         "Coverage" = coverage,
                         "Height"   = ut)
    
    return(c(results[[1]],results[[2]]))
  }
  if(shape1 > n){
    lt = qbeta(1-credint, shape1, shape2)
    ut = 1
    coverage = credint
    results = data_frame("Lower"    = lt,
                         "Upper"    = ut,
                         "Coverage" = coverage,
                         "Height"   = lt)
    
    return(c(results[[1]],results[[2]]))
  } else {
    hpdfunc <- function(h, shape1, shape2){
      mode = (shape1 - 1)/(shape1 + shape2 - 2)
      lt = uniroot(f=function(x){ dbeta(x,shape1, shape2) - h},
                   lower=0, upper=mode)$root
      ut = uniroot(f=function(x){ dbeta(x,shape1, shape2) - h},
                   lower=mode, upper=1)$root
      coverage = pbeta(ut, shape1, shape2) - pbeta(lt, shape1, shape2)
      
      hpdval = abs(credint-coverage)
      
      return(hpdval)
    }
    upper = max(dbeta(seq(0,1, by = 0.001), shape1, shape2)) 
    
    h = optimize(hpdfunc,
                 interval = seq(0,upper,by = 0.001),
                 lower = 0,
                 tol = .Machine$double.eps,
                 shape1,
                 shape2)
    
    h = h$minimum
    mode = (shape1 - 1)/(shape1 + shape2 - 2)
    lt = uniroot(f=function(x){ dbeta(x,shape1, shape2) - h},
                 lower=0, upper=mode)$root
    ut = uniroot(f=function(x){ dbeta(x,shape1, shape2) - h},
                 lower=mode, upper=1)$root
    coverage = pbeta(ut, shape1, shape2) - pbeta(lt, shape1, shape2)
    results = data_frame("Lower"    = lt,
                         "Upper"    = ut,
                         "Coverage" = coverage,
                         "Height"   = h)
    
    return(c(results[[1]],results[[2]]))
  }}

ni <- seq(0,100, by = 1)
p <- seq(0.0001,0.9999,1/1000)
a <- 0.05
testdata <- matrix(data = NA, nrow = length(ni),ncol = 3)
testdata[,1] <- sort(ni)
for(i in 1:nrow(testdata)){
  n = testdata[i,1]
  minvalues = vapply(p,blcover0,0, x = 0, one.sided = TRUE, credint = 1-a)
  testdata[i,2] = (which.min(minvalues)-1)/1000
  minvalues = vapply(p,jeffreyscover0, 0, x = 0, one.sided = TRUE, credint = 1-a)
  testdata[i,3] = (which.min(minvalues)-1)/1000
}
df <- tbl_df(testdata)
ggplot(data = df) +
  geom_line(aes(x = df$V1,y = df$V2), colour = "blue", group = "BL") +
  geom_line(aes(x = df$V1,y = df$V3), colour = "red", group = "Jef/Rat") +
  coord_cartesian(ylim = c(0,1),xlim =  c(0,100)) +
  labs(x="x_2",y="95% Upper Limit",
       title="95% HPD Upper Limit of BL (blue) and J/R (red) posteriors")
```

As we expect, the J/R is waited more towards 0 then the BL.

\newpage

```{r Q2Setup, include = FALSE}
rm(list = ls())
source("Question2/2setup.R")
library(ggthemes)
library(lme4)
library(ggtern)
require(lattice)
library(R2OpenBUGS)
opts_chunk$set(warning = FALSE, echo = TRUE, cache = TRUE)
opts_chunk$set(fig.align = "center")
options("digits" = 4)
```


## Question 2: Inference for variance components: 

(a) Use e.g. SAS (PROC VARCOMP) to perform a classical analysis of the data in Table 5.1.4 of Box & Tiao (1973), based on finding point estimates only.

I'll be using R. First, let's confirm the results reported by Box & Tiao (1973), using frequentist methods.

```{r anova}
data("Dyestuff2") #from the lme4 package
D2aov <- aov(Yield ~ Batch, Dyestuff2)
(D2summary <- summary(D2aov))
(sigma2_with <- D2summary[[1]]$`Mean Sq`[2])
(sigma2_btw <- (D2summary[[1]]$`Mean Sq`[1] - sigma2_with)/D2summary[[1]]$Df[1])
```

These results match what was reported by Box & Tiao (1973). Next, we can attempt to fit a linear model using the REML method.

```{r}
(D2REML <- lmer(Yield ~ 1 + (1|Batch), Dyestuff2, REML=TRUE))
D2REML <- broom::tidy(D2REML)
(sigma2_with <- D2REML[which(D2REML$group == "Residual"),"estimate"]^2)
(sigma2_btw <- D2REML[which(D2REML$group == "Batch"),"estimate"]^2)
```

Despite the fact that the result for $\sigma_{btw}$ is reported as $0$, we can observe some slight variation between batches using a dotplot (reordering the Batches to produce a smoother average line).

```{r}
dotplot(reorder(Batch, Yield) ~ Yield, Dyestuff2,
        ylab = "Batch", jitter.y = TRUE, aspect = 0.3,
        type = c("p","a"))
```

As a side note, the creator of the lme4 package that contains the Dyestuff2 data set explains the estimate of $0$ away by saying that \emph{"indicates that the level of
“between-group” variability is not sufficient to warrant incorporating random
effects in the model"} -- page 25 here: http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf.

(b)  Use WinBUGS for a Bayesian analysis of (a), and find reasonable point and interval estimates for $\sigma_1^2$ and $\sigma_2^2$. Include graphs, including one of the joint posterior. [6 marks] 

Before we begin, I've relabelled the ICC variable as ICC, instead of rho -- which I had previously -- just to keep things consistent. Also, I found methods for using the ICC method for models with more that two levels but didn't have time to go into them too much: I don't know if they automatically solve the issues we were discussing in class or not, although I assume they would.

I'll be using \textbf{OpenBUGS} and an \textbf{R} package called \textbf{R2OpenBUGS} to run these simulations.

```{r Question2b, cache = TRUE}
#define the model
dyesmodel <- function(){
  for(i in 1 : batches) {
    m[i] ~ dnorm(theta,  tau.btw)
    for(j in 1 : samples) {
     y[j , i] ~ dnorm(m[i], tau.with)
}}
  sigma2.with <- 1/tau.with
  tau.with ~ dgamma(0.001, 0.001)
  ICC ~ dunif(0,1)
  sigma2.btw <- sigma2.with*(ICC/(1-ICC))
  tau.btw <- 1/sigma2.btw
  theta ~ dflat()
}

# write the model code out to a file
write.model(dyesmodel, "dyesmodel.txt")
model.file1 = paste(getwd(),"dyesmodel.txt", sep="/")

#prepare the data for input into OpenBUGS
batches <- 6; samples <- 5
y <- tbl_df(Dyestuff2) %>% 
  mutate(samp = rep(seq(1,5,1),6)) %>%
  spread(Batch, Yield) %>%
  dplyr::select(-1)
y <- as.matrix(y)
  
data <- list ("batches","samples","y")

#initialization of variables
inits <- function(){list(theta=5, tau.with=1, ICC=0.5)}

#these are the parameters to save
parameters = c("sigma2.with","theta","sigma2.btw","ICC")

#run the model
dyes.sim <- bugs(
  data, inits, model.file = model.file1, parameters=parameters, 
  n.burnin = 1000, n.chains = 3, n.iter = 20000, 
  OpenBUGS.pgm=paste0("/Users/benjamin/Applications/wine/",
                      "drive_c/ProgramFiles/OpenBUGS/OpenBUGS323/OpenBUGS.exe"), 
  WINE="/opt/local/bin/wine", WINEPATH="/opt/local/bin/winepath", useWINE=T)
attach.bugs(dyes.sim)
kable(dyes.sim$summary, digits = getOption("digits"))
```

Here are the HPD intervals for each parameter.
```{r}
library(coda)
dye <- as.mcmc.list(dyes.sim)
HPDinterval(dye)[[1]]
```

```{r Q2bplot}

dyessimlist <- tbl_df(dyes.sim$sims.list)
densityplot(dyessimlist$sigma2.with, xlab = "Sigma2.with")
densityplot(dyessimlist$sigma2.btw, xlab = "Sigma2.btw")
densityplot(dyessimlist$theta, xlab = "theta")
densityplot(dyessimlist$ICC, xlab = "ICC")

df <- tbl_df(cbind(rBtw = dyessimlist$sigma2.btw, rWith = dyessimlist$sigma2.with))
ggplot(data = df, aes(x=rWith,y=rBtw)) + 
  geom_point(size=0.1, colour = "red", alpha = 0.1) +
  geom_density_2d() +
  coord_cartesian(xlim = c(0,30), ylim = c(0,15)) +
  theme_few() +
  labs(x="sigma2.btw",y="sigma2.with",
       title="Contours of the posterior distribution of (sigma2.btw, sigma2.with)")
```

(c) Box & Tiao also studied a 3-component model (Table 5.3.1). 
    i. Derive central credible intervals, for the 3 individual components, based on Table 5.3.3.
    ii. Use WinBUGS to do the same, and include graphs.
    iii. While not going as far as Box & Tiao’s Figure 5.3.2, produce a graph of the joint posterior of $\sigma_2^2$ and $\sigma_3^2$, and one of $\sigma_2^2 / \sigma_3^2$
    
Here is where my answers started to diverge somewhat more drastically from those reported by Box and Tiao, which I will discuss further on.

First, let's confirm the results in the text.

```{r Q2cAnova}
df <- readRDS("Question2/Data/Q2c.rds")
(BT3 <- summary(aov(Sample~(I/J/K),df)))
(m1 <- BT3[[1]]$`Mean Sq`[3])
(m2 <- (BT3[[1]]$`Mean Sq`[2]-BT3[[1]]$`Mean Sq`[3])/2)
(m3 <- (BT3[[1]]$`Mean Sq`[1]-BT3[[1]]$`Mean Sq`[2])/4)
(mAll <- m1+m2+m3)
(c(m1/mAll,m2/mAll,m3/mAll))
```

Next, the model for simulating in BUGS. My problems with the models that follow concern whether or not to simulate the mean of each level to use to centre the next. Here I have generated values for the mean of each Batch (I), Sample (J) and Test (K) and used them to generate samples from the joint posterior. In the next model I followed the model that (I believe?) was laid out by Box and Tiao and instead assumed that each mean was normally distributed about $0$. Either way, each model type -- applied to either problem -- seemed to give me very different results from Box et. als original analysis, hence my confusion.

If I am wrong in both cases, my only thought is that I have entered the data in reverse: i.e. that instead of a $10 x 2 x 2$ sampling scheme, I've actually entered a $2 x 2 X 10$. Anyway, I digress.

```{r Q2cmodel, cache = TRUE}
#define the model
threecompmodel <- function(){
	for( i in 1 : I ) {
		a[i] ~ dnorm(theta, tauI)
		for( j in 1 : J ) {
			b[i , j] ~ dnorm(a[i], tauJ)
			for( k in 1 : K ) {
				e[i , j , k] <- mu0 + a[i] + b[i , j]
				y[i , j , k] ~ dnorm(e[i , j , k], tauK)
}}}
  
	## Priors
  theta ~ dnorm(0.0,1.00E-04) # Batch Mean
	mu0   ~ dnorm(0.0,1.00E-04) # Sample Mean
	tauI  ~ dgamma(0.001, 0.001) # Between Batch variation
	tauJ  ~ dgamma(0.001, 0.001) # Between Sample variation
	tauK  ~ dgamma(0.001, 0.001) # Between Test variation
	sigma2I <- 1 / tauI
	sigma2J <- 1 / tauJ
	sigma2K <- 1 / tauK
	sigma2JK <- sigma2J / sigma2K
	denom <- sigma2I + sigma2J + sigma2K
	r1 <- sigma2I / denom
	r2 <- sigma2J / denom
	r3 <- sigma2K / denom
}
# write the model code out to a file
write.model(threecompmodel, "threecompmodelmodel.txt")
model.file.3comp = paste(getwd(),"threecompmodelmodel.txt", sep="/")

#prepare the data for input into OpenBUGS
  
data <- list(I=10,J=2,K=2,
             y = structure(
               .Data=c(2.004,  2.713,  0.602,  0.252,
                       4.342,  4.229,  3.344,  3.057,
                       0.869, -2.621, -3.896, -3.696,
                       3.531,  4.185,  1.722,  0.380,
                       2.579,  4.271, -2.101,  0.651,
                      -1.404, -1.003, -0.755, -2.202,
                      -1.676, -0.208, -9.139, -8.653,
                       1.670,  2.426,  1.834,  1.200,
                       2.141,  3.527,  0.462,  0.665,
                      -1.229, -0.596,  4.471,  1.606),
               .Dim=c(10,2,2)))

#initialization of variables
inits <- function(){
  list(mu0 = 0,theta=0,
       a = c(0,0,0,0,0,0,0,0,0,0),
	     b = structure(.Data = c(0,0,0,0,0,0,0,0,0,0,
	                             0,0,0,0,0,0,0,0,0,0),
	                   .Dim = c(10,2)),
       tauI = 1.0,tauJ = 1.0,tauK = 1.0)
}
#these are the parameters to save
parameters = c("sigma2I","sigma2J","sigma2K", "sigma2JK", "r1", "r2", "r3")

#run the model
threecomp.sim <- bugs(data, inits, model.file = model.file.3comp, parameters=parameters,
                      n.burnin = 1000, n.chains = 3, n.iter = 20000, 
                      OpenBUGS.pgm=paste0("/Users/benjamin/Applications/wine/",
                                          "drive_c/ProgramFiles/OpenBUGS/OpenBUGS323/OpenBUGS.exe"), 
                      WINE="/opt/local/bin/wine", WINEPATH="/opt/local/bin/winepath",useWINE=T)
attach.bugs(threecomp.sim)
kable(threecomp.sim$summary,digits = getOption("digits"))
```

Here are the HPD intervals for each parameter.
```{r}
library(coda)
threec <- as.mcmc.list(threecomp.sim)
HPDinterval(threec)[[1]]
```

```{r Q2cplot}
threecompsimlist <- tbl_df(threecomp.sim$sims.list)

densityplot(threecompsimlist$sigma2I, xlab = "Sigma2I")
densityplot(threecompsimlist$sigma2J, xlab = "Sigma2J")
densityplot(threecompsimlist$sigma2K, xlab = "Sigma2K")

df <- tbl_df(cbind(rBatches = threecompsimlist$r1,
                   rSamples = threecompsimlist$r2,
                   rTests = threecompsimlist$r3))

ggplot(data = threecompsimlist, aes(x=sigma2K,y=sigma2J)) + 
  geom_point(size=0.1, colour = "red", alpha = 0.1) +
  geom_density_2d() +
  coord_cartesian(xlim = c(0,20), ylim = c(0,1)) +
  theme_few() +
  labs(x="sigma2K",y="sigma2J",
       title="Contours of the posterior distribution of (sigma2J, sigma2K)")
```
```{r, fig.height=4,fig.width=8}
ggtern(data = df, aes(x=rBatches,z=rSamples,y=rTests)) + 
  theme_rgbw() +
  geom_point(size=0.1, colour = "red", alpha = 0.25) +
  geom_confidence_tern(breaks = c(0.95)) + 
  theme_clockwise() +
  labs(x="Batches",z="Samples",y="Tests",
       title="Contours of the posterior distribution of (r1, r2, r3)")
```

(d) Box, Hunter & Hunter (1976, Chapter 17.3) studied a pigment paste example with three components, focusing on point estimates only. Use WinBUGS again to perform a Bayesian analysis. Include graphs.

Again, any issues that are present in the model above will be reproduced here.

```{r Q2dmodel, cache = TRUE}
#define the model
pigmentmodel <- function(){
	for( i in 1 : I ) {
		a[i] ~ dnorm(0.0, tauI)
		for( j in 1 : J ) {
			b[i , j] ~ dnorm(0.0, tauJ)
			for( k in 1 : K ) {
				e[i , j , k] <- theta + a[i] + b[i , j]
				y[i , j , k] ~ dnorm(e[i , j , k], tauK)
  }}}
  
	## Priors
  theta ~ dflat() # sample mean
	tauI ~ dgamma(0.001, 0.001) # Between Batch variation
	tauJ ~ dgamma(0.001, 0.001) # Between Sample variation
	tauK ~ dgamma(0.001, 0.001) # Between Test variation
	sigma2I <- 1 / tauI
	sigma2J <- 1 / tauJ
	sigma2K <- 1 / tauK
	sigma2JK <- sigma2J / sigma2K
	denom <- sigma2I + sigma2J + sigma2K
	r1 <- sigma2I / denom
	r2 <- sigma2J / denom
	r3 <- sigma2K / denom
}
# write the model code out to a file
write.model(pigmentmodel, "dyes2model.txt")
model.file.pigment = paste(getwd(),"dyes2model.txt", sep="/")

#prepare the data for input into OpenBUGS
  
data <- list(I=15,J=2,K=2,
             y = structure(
               .Data=c(40,39,30,30,
                       26,28,25,26,
	                     29,28,14,15,
	                     30,31,24,24,
	                     19,20,17,17,
	                     33,32,26,24,
	                     23,24,32,33,
	                     34,34,29,29,
	                     27,27,31,31,
	                     13,16,27,24,
	                     25,23,25,27,
	                     29,29,31,32,
	                     19,20,29,30,
	                     23,23,25,25,
	                     39,37,26,28),
               .Dim=c(15,2,2)))

#initialization of variables
inits <- function(){
  list(theta=0,
       a = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),
	     b = structure(
	       .Data = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
	                 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),
	       .Dim = c(15,2)),
  tauI = 1.0,tauJ = 1.0,tauK = 1.0)
}

WINE="/opt/local/bin/wine"
WINEPATH="/opt/local/bin/winepath"
OpenBUGS.pgm=paste0("/Users/benjamin/Applications/wine/",
                    "drive_c/ProgramFiles/OpenBUGS/OpenBUGS323/OpenBUGS.exe")

#these are the parameters to save
parameters = c("sigma2I","sigma2J","sigma2K", "sigma2JK", "r1", "r2", "r3")

#run the model
pigment.sim <- bugs(
  data, inits, model.file = model.file.pigment, parameters=parameters,
  n.burnin = 1000, n.chains = 3, n.iter = 20000, 
  OpenBUGS.pgm=paste0("/Users/benjamin/Applications/wine/",
                      "drive_c/ProgramFiles/OpenBUGS/OpenBUGS323/OpenBUGS.exe"), 
  WINE="/opt/local/bin/wine", WINEPATH="/opt/local/bin/winepath", useWINE = T)
attach.bugs(pigment.sim)
kable(pigment.sim$summary,digits = getOption("digits"))
```

Here are the HPD intervals for each parameter.
```{r}
library(coda)
pig <- as.mcmc.list(pigment.sim)
HPDinterval(pig)[[1]]
```

```{r Q2dplot}

pigsimlist <- tbl_df(pigment.sim$sims.list)

densityplot(pigsimlist$sigma2I, xlab = "Sigma2I")
densityplot(pigsimlist$sigma2J, xlab = "Sigma2I")
densityplot(pigsimlist$sigma2K, xlab = "Sigma2I")

df <- tbl_df(cbind(rBatches = pigsimlist$r1, rSamples = pigsimlist$r2, rTests = pigsimlist$r3))
```
```{r, fig.height=4,fig.width=8}
ggtern(data=df,aes(x=rBatches,z=rSamples,y=rTests)) + 
  theme_rgbw() +
  geom_point(size=0.1, colour = "red") +
  geom_confidence_tern(breaks = c(0.5,0.7,0.9)) + 
  theme_clockwise() +
  labs(x="Batches",z="Samples",y="Tests",
       title="Contours of the posterior distribution of (r1, r2, r3)")
```
